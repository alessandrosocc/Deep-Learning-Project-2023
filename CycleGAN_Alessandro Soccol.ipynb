{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o4sd3vEnYqUH"
      },
      "source": [
        "## <center> Progetto Deep Learning - CycleGAN - Alessandro Soccol 60/79/00057</center>\n",
        "### <center>Abstract</center>\n",
        "Una cycleGAN non richiede un dataset di immagini accoppiate. Se vogliamo tradurre da arancie a mele, non abbiamo bisogno di un dataset di training formato da arancie che sono state convertite in mele. Questo permette di tradurre, per esempio, dipinti in fotografie. Tuttavia è un'architettura con molti limiti e utile in specifici casi molto poco complessi."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu6SHZRkYu5L"
      },
      "source": [
        "> **Informazione:** Questo notebook è frutto del tutorial in questo link https://machinelearningmastery.com/cyclegan-tutorial-with-keras/ e di articoli scientifici cui è fatta una panoramica a fine notebook e che ho usato per cercare di dare risposta a domande quali: perchè non è adatto al nostro task una CycleGAN? Quali alternative ci sono? Cosa avrei potuto fare per raggiungere gli obiettivi che mi ero prefissato?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_hFL0_i4qSmQ"
      },
      "source": [
        "### <center> Un minimo di teoria </center>\n",
        "L'architettura del modello è formata da due generatori, un generatore (A) genera immagini del primo dominio, il secondo generatore (B) genera immagini del secondo dominio. I generatori si occupano di fare una traduzione dell'immagine, ciò significa che la generazione dell'immagine è condizionata all'immagine di input che nel nostro caso è quella dell'altro dominio. Il generatore A, per esempio, prende un'immagine del dominio B in input e il generatore B prende un'immagine del dominio A in input.\\\n",
        "Ogni generatore ha un corrispondente modello discriminatore. Il discriminatore (A) prende le immagini reali del dominio (A) e immagini generate dal generatore (A) e predice se sono immagini reali o fake. Il secondo discriminatore (B) prende immagini reali del dominio B e immagini generate dal generatore (B) e predice se sono immagini reali o fake.\n",
        "\n",
        "    Domain-A -> Discriminator-A -> [Real/Fake]\n",
        "    Domain-B -> Generator-A -> Discriminator-A -> [Real/Fake]\n",
        "    Domain-B -> Discriminator-B -> [Real/Fake]\n",
        "    Domain-A -> Generator-B -> Discriminator-B -> [Real/Fake]\n",
        "\n",
        "Come una GAN, il generatore cerca di \"imbrogliare\" il discriminatore generando immagini che sono sempre più realistiche, mentre il discriminatore impara a rilevare meglio le immagini false.\\\n",
        "I generatori traducono versioni più ricostruite delle immagini di input dal dominio di origine e questo è fatto confrontando l'immagine di output del generatore con l'immagine originale. Il passaggio di un'immagine da entrambi i generatori è definito come ciclo. Ogni coppia di generatori è addestrata a riprodurre l'immagine di partenza, questo è definito come cycle-consistency.\n",
        "\n",
        "    Domain-B -> Generator-A -> Domain-A -> Generator-B -> Domain-B\n",
        "    Domain-A -> Generator-B -> Domain-B -> Generator-A -> Domain-A\n",
        "\n",
        "Una CycleGAN, per definizione, possiamo riassumerla con tre concetti fondamentali:\n",
        "- Unpaired image-to-image translation (Come scritto sopra, non abbiamo bisogno di un dataset di immagini accoppiate)\n",
        "- Adversarial Loss (Come nelle DCGAN tradizionali già viste a lezione)\n",
        "- Cycle-Consistency Loss\n",
        "\n",
        "![.](https://i.ibb.co/kQHJgRq/1.png)\\\n",
        "Il modello contiene due funzioni di mapping $G:X\\rightarrow Y$ e $F:Y\\rightarrow X$ con i rispettivi discriminatori $D_Y$ e $D_X$. $D_Y$ incoraggia $G$ a tradurre gli elementi del dominio $X$ in dati indistinguibili dal dominio $Y$, viceversa per $D_X$ e $F$. Per regolarizzare ulteriormente il modello, oltre all'adversarial loss, vengono introdotte due *cycle consistency loss* il cui obiettivo è fare in modo che se traduco da un dominio ad un altro, poi torno al dominio di partenza, devo arrivare al dato da cui sono partito. La cycle consistency loss si divide quindi in *forward cycle-consistency loss* in cui $x\\rightarrow G(x)\\rightarrow F(G(x))\\approx x$ (figura sopra (b)) e *backward cycle-consistency loss* in cui $y\\rightarrow F(y)\\rightarrow G(F(y))\\approx y$ (figura sopra (c)).\\\n",
        "**<center>Formalizzando</center>**\n",
        "L'obiettivo è imparare delle funzioni di mapping tra due domini $X$ ed $Y$, dati i loro esempi appartenenti al dominio, $\\{x_i\\}_{i=1}^N$ per cui $x_i\\in X$ e $\\{y_j\\}_{j=1}^M$ per cui $y_j\\in Y$. Il modello include due funzioni di mapping $G:X\\rightarrow Y$ e $F:Y\\rightarrow X$. In più vengono aggiunti due discriminatori $D_X$ e $D_Y$, in cui $D_X$ ha l'obiettivo di distinguere tra $y$ e le immagini tradotte $\\{F(y)\\}$, mentre $D_Y$ ha l'obiettivo di distinguere tra $\\{y\\}$ e $\\{G(x)\\}$.\\\n",
        "La funzione obiettivo contiene un'adversarial loss per fare il matching della distribuzione delle immagini generate con la distribuzione dei dati nel dominio target (Come una GAN) e una cycle consistency loss per fare in modo che i mapping imparati $G$ e $F$ non si contraddicano a vicenda.\n",
        "**<center>Adversarial Loss</center>**\n",
        "L'adversarial loss è applicata ad entrambe le funzioni di mapping, per esempio nel caso della funzione di mapping $G$ e il suo discriminatore $D_Y$, abbiamo $$\\mathcal{L}_{GAN}(G,D_Y,X,Y)=\\mathbb{E}_{y\\sim p_{data}(y)}[\\log D_Y(y)]+\\mathbb{E}_{x\\sim p_{data}(x)}[\\log (1-D_Y (G(x)))]$$\n",
        "In cui $G$ cerca di generare immagini $G(x)$ che sembrano simili alle immagini del dominio $Y$, mentre $D_Y$ ha l'obiettivo di distinguere tra le immagini tradotte $G(x)$ e gli esempi originali del dominio $y$. $G$ ha l'obiettivo di minimizzare la funzione mentre $D$ cerca di massimizzarla. Da quest'ultima frase si può intuire che l'obiettivo è cercare un equilibrio, non abbiamo una convergenza della funzione. quindi riassumento \n",
        "$$\\min_G \\max_{D_Y} \\mathcal{L}_{GAN}(G,D_Y,Y,X)$$\n",
        "Facciamo la stessa cosa per la funzione di mapping $F$ e il discriminatore $D_X$, quindi\n",
        "$$\\min_F \\max_{D_X} \\mathcal{L}_{GAN}(F,D_X,Y,X)$$\n",
        "**<center>Cycle Consistency Loss</center>**\n",
        "<center>Perchè usiamo un'altra loss? non potevamo usare l'adversarial loss e basta?</center>\n",
        "\n",
        "Quando una rete ha una grande capacità, può mappare lo stesso insieme di immagini di input ad una permutazione casuale di immagini nel dominio target in cui qualsiasi dei mapping imparati nel dominio target può indurre una distribuzione di output che corrisponde alla distribuzione target. Quindi l'adversarial loss da sola non può garantire che la funzione imparata possa mappare un input $x_i$ all'output desiderato $y_j$. Quindi per ridurre lo spazio delle possibili funzioni di mapping si vuole che le funzioni di mapping siano cycle-consistent, per ogni immagine $x$ del dominio $X$ il ciclo di traduzione dell'immagine dovrebbe essere in grado di riportare $x$ all'immagine originale, ossia $x\\rightarrow G(x)\\rightarrow F(G(x))\\approx x$, questa è la forward cycle consistency. Allo stesso modo, per ogni immagine $y$ del dominio $Y$, $G$ ed $F$ devono soddisfare la backward cycle consistency tale per cui $y\\rightarrow F(y)\\rightarrow G(F(y))\\approx y$.\\\n",
        "Questo viene fatto usando una cycle consistency loss \n",
        "$$\\mathcal{L}_{cyc}(G,F)=\\mathbb{E}_{x\\sim p_{data}(x)}[||F(G(x))-x||_1]+\\mathbb{E}_{y\\sim p_{data}(y)}[||G(F(y))-y||_1]$$\n",
        "\n",
        "Infine possiamo scrivere la funzione di loss finale come \n",
        "$$\\mathcal{L}(G,F,D_X,D_Y)=\\mathcal{L}_{GAN}(G,D_Y,X,Y)+\\mathcal{L}_{GAN}(F,D_X,Y,X)+\\lambda \\mathcal{L}_{cyc}(G,F)$$\n",
        "**<center>Limitazioni</center>**\n",
        "Una CycleGAN può fare cambiamenti minimi all'input, tant'è che la CycleGAN fallisce quando trova per esempio un cavallo con sopra una persona, oppure quando si vuole per esempio trasformare da cani a gatti. Un ricercatore giapponese è riuscito ad usare una cycleGAN ed avere risultati accettabili nel trasformare da gatti a cani, dice di aver usato una cycle loss più piccola (suppongo che intenda il valore di $\\lambda$ nella funzione di loss) ed un discriminatore locale+globale (suppongo il focus fosse su l'uso di un discriminatore locale che riesce a catturare informazioni locali come tratti della faccia di cani e gatti), tuttavia non ha rilasciato un'implementazione ne molte informazioni a riguardo. <a href=\"https://qiita.com/itok_msi/items/b6b615bc28b1a720afd7\">articolo ricercatore giapponese</a>\\\n",
        "![Fallimento CycleGAN](https://i.ibb.co/9ThDZVK/2.png)\n",
        "> **Informazione**: Le informazioni qua sopra, di teoria, sono prese dall'articolo originale. Per tenere il notebook il più snello possibile ho limitato la quantità di informazioni. \n",
        "> <a href=\"https://arxiv.org/abs/1703.10593\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwvheFBucLxh"
      },
      "source": [
        "**<center>Struttura della directory</center>**\n",
        "\n",
        "catvsdogs_cgan_small  \n",
        "-- *A -> cat  \n",
        "-- *B -> dog  \n",
        "\n",
        "La struttura del dataset deve essere cosi perchè ricordiamoci che passiamo da un Dominio A ad un dominio B e viceversa. é necessario definire bene i domini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOCns5Qrdl6a",
        "outputId": "dac1fdce-e887-4489-e012-6225af87dfce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\",force_remount=True) # force_remount mi serve perchè colab non si accorge che i file cambiano nel drive, in questo modo fa il remounting"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_P43i7xRDdGk"
      },
      "source": [
        "### <center>Informazioni Preliminari</center>\n",
        "Il colab si connetterà a google drive, è importante avere 2 directory nel proprio drive\n",
        "- Dataset\n",
        "- Checkpoint\n",
        "\n",
        "In checkpoint ci devono essere le cartelle dei pesi dei vari modelli. Devono essere 6.\\\n",
        "In dataset ci deve essere il file `catsvsdogs_64.npz`\n",
        "> **Importante**: All'interno di ogni cartella riguardante i pesi di un modello, ci sono file \"nascosti\" che iniziano con il punto. Assicurarsi che quei file siano presenti nelle cartelle dei pesi di ogni modello, altrimenti darà errore. \n",
        "> **Copiando ed incollando la cartella nel drive potrebbe non copiare questi file!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hQ146NcyDzBC"
      },
      "outputs": [],
      "source": [
        "# Può creare le cartelle cosi. è necessario inserire manualmente i file richiesti all'interno accedendo a google drive nel link nel file README.txt\n",
        "!mkdir /content/drive/MyDrive/Dataset\n",
        "!mkdir /content/drive/MyDrive/Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ner7ZiM-ERVW"
      },
      "source": [
        "Una volta eseguita la cella sopra per la prima volta, è necessario caricare i file che trova in allegato sui pesi dei modelli e sul dataset compresso!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7J5Za6ClcLxj"
      },
      "outputs": [],
      "source": [
        "training_phase=True # Vuole addestrare? allora a True.\n",
        "create_compressed_dataset=False # Vuoi creare il dataset in forma compressa (.npz)? [Richiede un pò di tempo!]\n",
        "load_trained_models=False # Vuoi caricare un modello addestrato precedentemente?\n",
        "show_some_data=False # Mostrami alcuni dati del dataset a caso. Non voglio perchè mi rallenta e basta l'esecuzione totale del codice\n",
        "iterazione=\"050600\" # iterazione del modello che stai caricando, da impostare (e utile) solo se load_trained_models è true, andrà a cercare il file dell'iterazione che hai scelto su gdrive\n",
        "loaded=0 # quando carichi il modello, non riprende ad iterare da 0 ma da dove si era fermato, viene modificato in seguito, non toccare!\n",
        "directory=\"/content/drive/MyDrive/\" # gdrive directory\n",
        "directoryDataset=\"/content/drive/MyDrive/Dataset/\" # gdrive directory dataset\n",
        "directoryCheckpoint=\"/content/drive/MyDrive/Checkpoint/\" # gdrive checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v5P4dxaBcLxm"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "from keras.utils import img_to_array\n",
        "from keras.utils import load_img\n",
        "from numpy import savez_compressed\n",
        "\n",
        "# carico tutte le immagini in una directory, in memoria\n",
        "def load_images(path, size=(64,64)):\n",
        "    data_list = list()\n",
        "    # enumerate filenames in directory, assume all are images\n",
        "    for filename in listdir(path):\n",
        "        # load and resize the image\n",
        "        pixels = load_img(path + filename, target_size=size)\n",
        "        # convert to numpy array\n",
        "        pixels = img_to_array(pixels)\n",
        "        # store\n",
        "        data_list.append(pixels)\n",
        "    return np.asarray(data_list)\n",
        "\n",
        "if create_compressed_dataset:\n",
        "    # dataset path\n",
        "    path = directoryDataset+\"catsvsdogs_cgan_small_CycleGAN/\"\n",
        "    # load dataset A\n",
        "    dataA1 = load_images(path + 'trainA/')\n",
        "    dataAB = load_images(path + 'testA/')\n",
        "    dataA = vstack((dataA1, dataAB)) # Notare come usiamo tutti i dati di training che abbiamo! non ci interessa se sono di test. In quanto usiamo il modello successivamente per aumentare i dati e passare da un dominio ad un altro\n",
        "    # vstack è una concatenazione dei dati sul primo asse.\n",
        "    print('Loaded dataA: ', dataA.shape)\n",
        "    # load dataset B\n",
        "    dataB1 = load_images(path + 'trainB/')\n",
        "    dataB2 = load_images(path + 'testB/')\n",
        "    dataB = vstack((dataB1, dataB2)) # Notare come usiamo tutti i dati di training che abbiamo! non ci interessa se sono di test\n",
        "    # vstack è una concatenazione dei dati sul primo asse.\n",
        "    print('Loaded dataB: ', dataB.shape)\n",
        "    # save as compressed numpy array\n",
        "    filename = f'{directoryDataset}catsvsdogs_64.npz' # comprimiamo il dataset per usarlo meglio con Numpy, infatti successivamente lo caricheremo con load di numpy\n",
        "    savez_compressed(filename, dataA, dataB) # creiamo il dataset in maniera compressa .npz e lo salviamo \n",
        "    print('Saved dataset: ', filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3Zc4303uKa3"
      },
      "source": [
        "Mostriamo alcuni dati, solo se `show_some_data` definita prima è True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gk0axwOjcLxp"
      },
      "outputs": [],
      "source": [
        "from numpy import load\n",
        "from matplotlib import pyplot\n",
        "if show_some_data:\n",
        "    # load and plot the prepared dataset\n",
        "    # load the dataset\n",
        "    data = load(f'{directoryDataset}catsvsdogs_64.npz')\n",
        "    dataA, dataB = data['arr_0'], data['arr_1']\n",
        "    print('Loaded: ', dataA.shape, dataB.shape)\n",
        "    # plot source images\n",
        "    n_samples = 3\n",
        "    for i in range(n_samples):\n",
        "        pyplot.subplot(2, n_samples, 1 + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(dataA[i].astype('uint8'))\n",
        "    # plot target image\n",
        "    for i in range(n_samples):\n",
        "        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(dataB[i].astype('uint8'))\n",
        "    pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlfQEdymcLxr"
      },
      "source": [
        "## <center> Costruzione della CycleGAN </center>\n",
        "`tensorflow_addons` è necessario per usare un layer chiamato InstanceNormalization non incluso in tensorflow classico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUrgpZoofa-J",
        "outputId": "fa073d70-4e3f-439a-91da-6f2c64ccca30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKlmWjvgvHwo"
      },
      "source": [
        "La instanceNormalization è uguale alla BatchNormalization vista a lezione, solamente anzichè considerare un batch consideriamo le singole istanze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO8zeosWcLxs",
        "outputId": "d4465ebc-04fb-448d-aabb-9e36c6d49d5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "# define layer\n",
        "layer = InstanceNormalization(axis=-1) # axis = -1 per fare in modo che le caratteristiche siano normalizzate per ogni feature map."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nknQx-7A51hp"
      },
      "source": [
        "### <center> Discriminatore </center>\n",
        "Notare i blocchi formati da layer convolutivi, InstanceNormalization e LeakyReLU.\\\n",
        "La notazione C64, C128, C256 ecc. è definita come segue:\n",
        "- `C` indica un blocco formato da Convoluzione, BatchNormalizatione e Leaky ReLU\n",
        "- il numero subito dopo indica il numero di filtri\n",
        "- I layer convolutivi, quando scriviamo quella notazione, hanno una kernel size 4x4 e uno stride 2x2\n",
        "Più info sulla notazione: <a href=\"https://arxiv.org/pdf/1611.07004.pdf\">Image-to-Image Translation with Conditional Adversarial Networks ; Appendix 6, (6.1) </a> è una notazione molto usata quando si usano modelli con blocchi di questo tipo; l'ho ritrovata in diversi articoli. Può variare ed essere per esempio definita come `c7s1-k` in cui abbiamo un blocco formato da Conv-InstanceNorm-ReLU 7x7 con `k` filtri e stride 1, `dk` indica un blocco come scritto prima ma 3x3 con `k`filtri e stride 2. e altro che si può trovare nell'articolo <a href=\"https://arxiv.org/pdf/2002.10102.pdf\">GANHopper, sezione 3.1</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oa92R_xfcLxt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# define the discriminator model\n",
        "def define_discriminator(image_shape):\n",
        "    # weight initialization\n",
        "    init = tf.keras.initializers.RandomNormal(stddev=0.02,seed=1337)\n",
        "    # source image input\n",
        "    in_image = tf.keras.Input(shape=image_shape)\n",
        "    # C64\n",
        "    d = tf.keras.layers.Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
        "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
        "    # C128\n",
        "    d = tf.keras.layers.Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "    d = InstanceNormalization(axis=-1)(d)\n",
        "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
        "    # C256\n",
        "    d = tf.keras.layers.Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "    d = InstanceNormalization(axis=-1)(d)\n",
        "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
        "    # C512\n",
        "    d = tf.keras.layers.Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "    d = InstanceNormalization(axis=-1)(d)\n",
        "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
        "    # second last output layer\n",
        "    d = tf.keras.layers.Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "    d = InstanceNormalization(axis=-1)(d)\n",
        "    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n",
        "    # patch output\n",
        "    patch_out = tf.keras.layers.Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "    # define model\n",
        "    model = tf.keras.Model(in_image, patch_out)\n",
        "    # compile model\n",
        "\n",
        "\n",
        "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2iaL9BwyZo8"
      },
      "source": [
        "### <center> Generatore </center>\n",
        "Il generatore ha un'architettura encoder-decoder. Il modello prende un'immagine reale come per esempio un cane e genera un'immagine di un gatto. Fa questo facendo prima di tutto il downsampling o l'encoding dell'immagine facendola passare per un collo di bottiglia, dopodiché interpreta la codifica con un numero di connessioni residue (ResNet) seguita da una serie di layer che fanno l'upsampling o la decodifica della rappresentazione, alla dimensione dell'immagine che ci serve in output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATvqxLCf53o0"
      },
      "source": [
        "Definiamo un blocco residual network, visto a lezione. Questi blocchi formati da due layer CNN 3x3 il cui input del blocco è concatenato all'output del blocco.\\\n",
        "Crea due blocchi Convoluzione-InstanceNormalization con filtri 3x3 e stride 1x1 senza un'attivazione ReLU dopo il secondo blocco."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REBnRvze0VCO"
      },
      "source": [
        "Il generatore è addestrato tramite il relativo modello discriminatore. I generatori sono addestrati per fare in modo di minimizzare la loss predetta dal discriminatore per le immagini generate, identificate come reali. Questo è fatto con l'adversarial loss, in questo modo il generatore è spinto a generare immagini sempre migliori.\\\n",
        "Il generatore è inoltre aggiornato sulla base di quanto è efficace nella generazione dell'immagine di input quando viene usato con gli altri generatori, da qua viene il cycle-loss.\\\n",
        "Infine un generatore restituisce un'immagine in output senza traduzione quando gli viene data un'immagine del dominio di destinazione, questo avviene grazie all'identity loss.\\\n",
        "Ogni generatore è ottimizzato attraverso la combinazione di quattro output con 4 funzioni di loss\n",
        "\n",
        "    - Adversarial loss (L2 or mean squared error).\n",
        "    - Identity loss (L1 or mean absolute error).\n",
        "    - Forward cycle loss (L1 or mean absolute error).\n",
        "    - Backward cycle loss (L1 or mean absolute error).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JBTnX_I-cLxu"
      },
      "outputs": [],
      "source": [
        "# generator a Residual Network block\n",
        "def resnet_block(n_filters, input_layer):\n",
        "    # weight initialization\n",
        "    init = tf.keras.initializers.RandomNormal(stddev=0.02,seed=1337)\n",
        "    # first layer convolutional layer\n",
        "    g = tf.keras.layers.Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    g = tf.keras.layers.Activation('relu')(g)\n",
        "    # second convolutional layer\n",
        "    g = tf.keras.layers.Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    # concatenate merge channel-wise with input layer\n",
        "    g = tf.keras.layers.Concatenate()([g, input_layer])\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NRA3bJHRcLxx"
      },
      "outputs": [],
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(image_shape, n_resnet=9):\n",
        "    # weight initialization\n",
        "    init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
        "    # image input\n",
        "    in_image = tf.keras.Input(shape=image_shape)\n",
        "    # c7s1-64\n",
        "    g = tf.keras.layers.Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    g = tf.keras.layers.Activation('relu')(g)\n",
        "    # d128\n",
        "    g = tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    g = tf.keras.layers.Activation('relu')(g)\n",
        "    # d256\n",
        "    g = tf.keras.layers.Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    g = tf.keras.layers.Activation('relu')(g)\n",
        "    # R256\n",
        "    # Residual Network\n",
        "    for _ in range(n_resnet):\n",
        "        g = resnet_block(256, g)\n",
        "    # u128\n",
        "    g = tf.keras.layers.Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    g = tf.keras.layers.Activation('relu')(g)\n",
        "    # u64\n",
        "    g = tf.keras.layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    g = tf.keras.layers.Activation('relu')(g)\n",
        "    # c7s1-3\n",
        "    g = tf.keras.layers.Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n",
        "    g = InstanceNormalization(axis=-1)(g)\n",
        "    out_image = tf.keras.layers.Activation('tanh')(g)\n",
        "    # define model\n",
        "    model = tf.keras.Model(in_image, out_image)\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kp7Uwn886AsQ"
      },
      "source": [
        "### <center> Composite model </center>\n",
        "Per addestrare i generatori e discriminatori definiamo un modello che al suo interno ha i generatori e discriminatori. Questo è fatto per addestrare ogni generatore il quale è responsabile di aggiornare i suoi pesi anche se è necessario condividere i pesi con il suo discriminatore e l'altro generatore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeGOsoZP3rq0"
      },
      "source": [
        "Il discriminatore è connesso all'output del generatore per fare in modo che classifichi le immagini generate come reali o fake. Un secondo input del composite model è definito come un'immagine dal dominio target, anzichè il dominio sorgente, che il generatore si aspetta di dare in output senza la traduzione per l'identity mapping. Successivamente la forward cycle loss comporta il collegamento dell'uscita del generatore all'altro generatore che ricostruirà l'immagine di partenza. Infine, la backward cycle loss coinvolge l'immagine del dominio di destinazione usata per l'identity mapping che viene fatta passare anche attraverso l'altro generatore, la cui uscita è collegata al nostro generatore principale come ingresso e produce una versione ricostruita di quell'immagine dal dominio di partenza.\\\n",
        "In sintesi, il composite model ha due input per le immagini reali del dominio A e B, quattro uscite per l'output del discriminatore ossia l'immagine generata dall'identity, l'immagine generata dal forward cycle e l'immagine generata dal backward cycle.\\\n",
        "Per il composite model vengono aggiornati solo i pesi del primo modello o del generatore principale e ciò avviene tramite la somma ponderata di tutte le funzioni di perdita. Alla perdita del ciclo viene attribuito un peso maggiore (10) rispetto alla perdita avversaria, come descritto nel paper originale, e la perdita dell'identità viene sempre utilizzata con una ponderazione pari alla metà di quella della perdita del ciclo (5). Come definito nell'implementazione del paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R16fXUn-56xf"
      },
      "source": [
        "**<center>Cosa ci facciamo con il composite model?</center>**\\\n",
        "Dobbiamo creare un composite model per ogni generatore, per esempio per cani verso gatti e per gatti verso cani.\\\n",
        "\\\n",
        "*Generatore A - Composite Model da Cani (B) a Gatti (A)*\n",
        "\n",
        "    Adversarial Loss: Domain-B -> Generator-A -> Domain-A -> Discriminator-A -> [real/fake]\n",
        "    Identity Loss: Domain-A -> Generator-A -> Domain-A\n",
        "    Forward Cycle Loss: Domain-B -> Generator-A -> Domain-A -> Generator-B -> Domain-B\n",
        "    Backward Cycle Loss: Domain-A -> Generator-B -> Domain-B -> Generator-A -> Domain-A\n",
        "\n",
        "*Generatore B - Composite model da Gatti (A) a Cani (B)*\n",
        "\n",
        "    Adversarial Loss: Domain-A -> Generator-B -> Domain-B -> Discriminator-B -> [real/fake]\n",
        "    Identity Loss: Domain-B -> Generator-B -> Domain-B\n",
        "    Forward Cycle Loss: Domain-A -> Generator-B -> Domain-B -> Generator-A -> Domain-A\n",
        "    Backward Cycle Loss: Domain-B -> Generator-A -> Domain-A -> Generator-B -> Domain-B\n",
        "\n",
        "\n",
        "> Ricordiamoci che nelle prime celle il dominio A corrisponde ai gatti e il dominio B ai cani!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w93tDKn8cLxx"
      },
      "outputs": [],
      "source": [
        "def define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n",
        "    # ensure the model we're updating is trainable\n",
        "    g_model_1.trainable = True\n",
        "    # mark discriminator as not trainable\n",
        "    d_model.trainable = False\n",
        "    # mark other generator model as not trainable\n",
        "    g_model_2.trainable = False\n",
        "    # discriminator element\n",
        "    input_gen = tf.keras.Input(shape=image_shape)\n",
        "    gen1_out = g_model_1(input_gen)\n",
        "    output_d = d_model(gen1_out)\n",
        "    # identity element\n",
        "    input_id = tf.keras.Input(shape=image_shape)\n",
        "    output_id = g_model_1(input_id)\n",
        "    # forward cycle\n",
        "    output_f = g_model_2(gen1_out)\n",
        "    # backward cycle\n",
        "    gen2_out = g_model_2(input_id)\n",
        "    output_b = g_model_1(gen2_out)\n",
        "    # define model graph\n",
        "    model = tf.keras.Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
        "    # define optimization algorithm configuration\n",
        "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    # compile model with weighting of least squares loss and L1 loss\n",
        "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I-cB51a6NFM"
      },
      "source": [
        "Funzione per caricare i dati di training, vengono scalati tra -1 e 1; scalare i dati è una best practice che si trova in tutti i paper riguardanti le GAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fUBMU2PjcLxy"
      },
      "outputs": [],
      "source": [
        "# load and prepare training images\n",
        "def load_real_samples(filename):\n",
        "    # load the dataset\n",
        "    data = load(filename)\n",
        "    # unpack arrays\n",
        "    X1, X2 = data['arr_0'], data['arr_1']\n",
        "    # scale from [0,255] to [-1,1]\n",
        "    X1 = (X1 - 127.5) / 127.5\n",
        "    X2 = (X2 - 127.5) / 127.5\n",
        "    return [X1, X2]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx-PsqoP6TdR"
      },
      "source": [
        "`generate_real_samples` prende un numpy array per un dominio input e restituisce il numero di immagini in maniera random che si vuole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OazhPkvlcLxz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "    # choose random instances\n",
        "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "    # retrieve selected images\n",
        "    X = dataset[ix]\n",
        "    # generate 'real' class labels (1)\n",
        "    y = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
        "    return X, y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k6SOg-jW8AKC"
      },
      "source": [
        "`generate_fake_samples` genera un campione, dato un generatore e il campione di immagini reali dal dominio sorgente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Fho9vAgcLx0"
      },
      "outputs": [],
      "source": [
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, dataset, patch_shape):\n",
        "    # generate fake instance\n",
        "    X = g_model.predict(dataset)\n",
        "    # create 'fake' class labels (0)\n",
        "    y = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYO-RUpl6n-Y"
      },
      "source": [
        "`save_models` salva i pesi dei modelli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xpYgI_e1cLx0"
      },
      "outputs": [],
      "source": [
        "# salvo tutti i pesi del modello nel file, lo faccio per caricare i dati e continuare l'addestramento da dove viene interrotto per google colab\n",
        "def save_models(step, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA,d_model_AtoB, d_model_BtoA):\n",
        "    # save the first generator model\n",
        "    filename1 = f'{directory}Checkpoint/g_model_AtoB_%06d_tf/' % (step+1)\n",
        "    g_model_AtoB.save_weights(filename1)\n",
        "    # save the second generator model\n",
        "    filename2 = f'{directory}Checkpoint/g_model_BtoA_%06d_tf/' % (step+1)\n",
        "    g_model_BtoA.save_weights(filename2)\n",
        "    # save the third generator model\n",
        "    filename3 = f'{directory}Checkpoint/c_model_AtoB_%06d_tf/' % (step+1)\n",
        "    c_model_AtoB.save_weights(filename3)\n",
        "    # save the fourth generator model\n",
        "    filename4 = f'{directory}Checkpoint/c_model_BtoA_%06d_tf/' % (step+1)\n",
        "    c_model_BtoA.save_weights(filename4)\n",
        "    # save the fifth generator model\n",
        "    filename5 = f'{directory}Checkpoint/d_model_A_%06d_tf/' % (step+1)\n",
        "    d_model_AtoB.save_weights(filename5)\n",
        "    # save the sixth generator model\n",
        "    filename6 = f'{directory}Checkpoint/d_model_B_%06d_tf/' % (step+1)\n",
        "    d_model_BtoA.save_weights(filename6)\n",
        "    print('>Saved: %s %s %s %s %s %s' % (filename1, filename2,filename3,filename4, filename5,filename6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9MC6K4m6qbH"
      },
      "source": [
        "`summarize_performance` mostra come vengono traslate le immagini da un dominio ad un altro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GXG8lMB5cLx1"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
        "    # select a sample of input images\n",
        "    X_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
        "    # generate translated images\n",
        "    X_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
        "    # scale all pixels from [-1,1] to [0,1]\n",
        "    X_in = (X_in + 1) / 2.0\n",
        "    X_out = (X_out + 1) / 2.0\n",
        "    # plot real images\n",
        "    for i in range(n_samples):\n",
        "        pyplot.subplot(2, n_samples, 1 + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(X_in[i])\n",
        "    # plot translated image\n",
        "    for i in range(n_samples):\n",
        "        pyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(X_out[i])\n",
        "    # save plot to file\n",
        "    filename1 = f'{directory}Checkpoint/%s_generated_plot_%06d.png' % (name, (step+1))\n",
        "    pyplot.savefig(filename1)\n",
        "    pyplot.close()\n",
        "    print(\"IMMAGINE SALVATA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HPAVFic9oSp"
      },
      "source": [
        "`update_image_pool` serve per creare un pool di 50 immagini che vengono cambiate di volta in volta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Bm_eddx9cLx1"
      },
      "outputs": [],
      "source": [
        "from random import random\n",
        "# update image pool for fake images\n",
        "def update_image_pool(pool, images, max_size=5):\n",
        "\tselected = list()\n",
        "\tfor image in images:\n",
        "\t\tif len(pool) < max_size:\n",
        "\t\t\t# stock the pool\n",
        "\t\t\tpool.append(image)\n",
        "\t\t\tselected.append(image)\n",
        "\t\telif random() < 0.5:\n",
        "\t\t\t# use image, but don't add it to the pool\n",
        "\t\t\tselected.append(image)\n",
        "\t\telse:\n",
        "\t\t\t# replace an existing image and use replaced image\n",
        "\t\t\tix = np.random.randint(0, len(pool))\n",
        "\t\t\tselected.append(pool[ix])\n",
        "\t\t\tpool[ix] = image\n",
        "\treturn asarray(selected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ILWoLlO6vku"
      },
      "source": [
        "Funzione di Training. La `batch_size` è impostata ad un'immagine come nel paper originale.\\\n",
        "Viene preso un batch di immagini reali da ogni dominio, viene generato un batch di immagini da ogni dominio. Le immagini fake sono usate per aggiornare il pool di immagini fake di ogni discriminatore.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Dxn0mVmwcLx2"
      },
      "outputs": [],
      "source": [
        "# train cyclegan models\n",
        "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset, loaded):\n",
        "\t# define properties of the training run\n",
        "\tn_epochs, n_batch, = 100, 1 # 100 epoche, batch di una immagine ciascuno\n",
        "\t# determine the output square shape of the discriminator\n",
        "\tn_patch = d_model_A.output_shape[1]\n",
        "\tprint(\"N_PATH - TRAIN FUCN\",d_model_A.output_shape)\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# prepare image pool for fakes\n",
        "\tpoolA, poolB = list(), list()\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tbat_per_epo = int(len(trainA) / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# select a batch of real samples\n",
        "\t\tX_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
        "\t\tX_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
        "\t\t# generate a batch of fake samples\n",
        "\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
        "\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
        "\t\t# update fakes from pool\n",
        "\t\tX_fakeA = update_image_pool(poolA, X_fakeA)\n",
        "\t\tX_fakeB = update_image_pool(poolB, X_fakeB)\n",
        "\t\t# update generator B->A via adversarial and cycle loss\n",
        "\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
        "\t\tg_loss2*=0.1\n",
        "\t\t# update discriminator for A -> [real/fake]\n",
        "\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
        "\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
        "\t\t# update generator A->B via adversarial and cycle loss\n",
        "\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
        "\t\tg_loss1*=0.1\n",
        "\t\t# update discriminator for B -> [real/fake]\n",
        "\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
        "\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
        "\t\t# summarize performance\n",
        "\t\tprint('[ iterazione -> %d | epoch -> %d ] dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+loaded+1,(i+loaded+1)/len(trainA), dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
        "\t\t# evaluate the model performance\n",
        "\t\tif (i+1) % (bat_per_epo * 1) == 0:\n",
        "\t\t\t# plot A->B translation\n",
        "\t\t\tsummarize_performance(i+loaded, g_model_AtoB, trainA, 'AtoB')\n",
        "\t\t\t# plot B->A translation\n",
        "\t\t\tsummarize_performance(i+loaded, g_model_BtoA, trainB, 'BtoA')\n",
        "\t\tif (i+1) % (bat_per_epo * 1) == 0:\n",
        "\t\t\tprint(\"\\n\\nSto Salvando i pesi \\n\\n\")\n",
        "\t\t\tif load_trained_models:\n",
        "\t\t\t\t# Salviamo i modelli per poterli ricaricare.\n",
        "\t\t\t\tsave_models(i+loaded, g_model_AtoB, g_model_BtoA, d_model_A, d_model_B,c_model_AtoB,c_model_BtoA)\n",
        "\t\t\telse:\n",
        "\t\t\t\tsave_models(i, g_model_AtoB, g_model_BtoA, d_model_A, d_model_B,c_model_AtoB,c_model_BtoA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8eakUdX6xjo"
      },
      "source": [
        "Definizione generatori, discriminatori e caricamento dei dati. Nel caso in cui ci fossero dei pesi da caricare, vengono definiti i modelli e gli vengono caricati i pesi. Ho scelto di salvare e caricare i pesi perchè mi sembrava più intuitivo da capire, piuttosto che salvarmi i modelli nel formato .h5 che da problemi (dice che il modello non è compilato)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "bgmcbFQNcLx3",
        "outputId": "7251e2f5-3620-4819-bd8b-31b01a2a5bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded (2200, 64, 64, 3) (2200, 64, 64, 3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N_PATH - TRAIN FUCN (None, 4, 4, 1)\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "[ iterazione -> 1 | epoch -> 0 ] dA[0.812,0.479] dB[1.068,0.548] g[1.897,1.866]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[ iterazione -> 2 | epoch -> 0 ] dA[2.984,0.536] dB[0.582,0.492] g[1.724,1.912]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[ iterazione -> 3 | epoch -> 0 ] dA[0.438,0.859] dB[0.692,1.221] g[2.094,1.909]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[ iterazione -> 4 | epoch -> 0 ] dA[0.429,0.796] dB[2.304,0.913] g[1.657,1.675]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[ iterazione -> 5 | epoch -> 0 ] dA[1.454,0.438] dB[1.256,0.600] g[1.699,1.703]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[ iterazione -> 6 | epoch -> 0 ] dA[0.686,12.632] dB[1.191,1.055] g[1.681,1.662]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[ iterazione -> 7 | epoch -> 0 ] dA[1.695,24.127] dB[1.069,18.075] g[1.868,1.775]\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-003ed4a5c7e7>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# train models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtraining_phase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# magari non voglio fare il training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_AtoB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_BtoA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model_AtoB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model_BtoA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-74eb181e2b35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset, loaded)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mdA_loss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fakeA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fakeA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# update generator A->B via adversarial and cycle loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mg_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_model_AtoB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_realB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mg_loss1\u001b[0m\u001b[0;34m*=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# update discriminator for B -> [real/fake]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from numpy import load\n",
        "dataset = load_real_samples(f'{directoryDataset}catsvsdogs_64.npz') # da modificare, bisogna ricreare il dataset\n",
        "# Carico immagini\n",
        "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
        "# definisco la shape dell'input sulla base del dataset caricato\n",
        "image_shape = dataset[0].shape[1:]\n",
        "# generator: A -> B\n",
        "g_model_AtoB = define_generator(image_shape)\n",
        "# generator: B -> A\n",
        "g_model_BtoA = define_generator(image_shape)\n",
        "# discriminator: A -> [real/fake]\n",
        "d_model_A = define_discriminator(image_shape)\n",
        "# discriminator: B -> [real/fake]\n",
        "d_model_B = define_discriminator(image_shape)\n",
        "# composite: A -> B -> [real/fake, A]\n",
        "c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n",
        "# composite: B -> A -> [real/fake, B]\n",
        "c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n",
        "if load_trained_models: # carico modello precedente e riparto da dove mi sono fermato, usato perchè cosi se cadono le sessioni di colab free riesco a ricaricarle e ripartire\n",
        "    loaded=int(iterazione)\n",
        "    g_model_AtoB.load_weights(f'{directoryCheckpoint}g_model_AtoB_{iterazione}_tf/')\n",
        "    g_model_BtoA.load_weights(f'{directoryCheckpoint}g_model_BtoA_{iterazione}_tf/')\n",
        "    d_model_A.load_weights(f'{directoryCheckpoint}d_model_A_{iterazione}_tf/')\n",
        "    d_model_B.load_weights(f'{directoryCheckpoint}d_model_B_{iterazione}_tf/')\n",
        "    c_model_AtoB.load_weights(f'{directoryCheckpoint}c_model_AtoB_{iterazione}_tf/')\n",
        "    c_model_BtoA.load_weights(f'{directoryCheckpoint}c_model_BtoA_{iterazione}_tf/')\n",
        "\n",
        "# Addestramento del modello, solo se training_phase è a true!\n",
        "if training_phase: # magari non voglio fare il training!\n",
        "  train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset, loaded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov4I03nrAk78"
      },
      "source": [
        "## <center>Traduciamo le immagini da un dominio ad un altro</center>\n",
        "Solamente dopo aver fatto l'addestramento o dopo aver caricato i pesi del modello!\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bS_Y-mc3A1cn"
      },
      "source": [
        "### <center>Carichiamo il dataset</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DGeMhL4A8K6"
      },
      "outputs": [],
      "source": [
        "A_data, B_data = load_real_samples(f'{directoryDataset}catsvsdogs_64.npz') # prendo dei dati dal dataset\n",
        "print('Loaded', A_data.shape, B_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGelr4OGBgZ5"
      },
      "source": [
        "### <center>Definiamo e carichiamo i pesi dei modelli</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPed-gVoB-Sn"
      },
      "outputs": [],
      "source": [
        "# generator: A -> B\n",
        "model_AtoB = define_generator(image_shape)\n",
        "# generator: B -> A\n",
        "model_BtoA = define_generator(image_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhoqSPo9Bfxe"
      },
      "outputs": [],
      "source": [
        "model_AtoB.load_weights(f'{directoryCheckpoint}g_model_AtoB_{iterazione}_tf/')\n",
        "model_BtoA.load_weights(f'{directoryCheckpoint}g_model_BtoA_{iterazione}_tf/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR-ec9urC8aL"
      },
      "source": [
        "### <center>Testiamo la cycleGAN da un dominio all'altro e viceversa</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ph1GMrHCL7P"
      },
      "source": [
        "Creiamo una funzione che prende un'immagine qualsiasi nel dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_zWNCdRCQ_t"
      },
      "outputs": [],
      "source": [
        "def select_sample(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX = dataset[ix]\n",
        "\treturn X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb0qVAeMCaEg"
      },
      "source": [
        "Scegliamo un'immagine random dal dominio A (Gatti), per tradurla nel dominio B (Cani) usando il generatore A -> B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG3xRf1yChN-"
      },
      "outputs": [],
      "source": [
        "# plot A->B->A\n",
        "A_real = select_sample(A_data, 1)\n",
        "B_generated  = model_AtoB.predict(A_real)\n",
        "A_reconstructed = model_BtoA.predict(B_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqIQuOoCrlG"
      },
      "source": [
        "Facciamo il plot delle tre immagini: immagine reale, immagine tradotta nell'altro dominio, immagine ricostruita nel dominio originale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7V4wbJjCy6C"
      },
      "outputs": [],
      "source": [
        "# plot the image, the translation, and the reconstruction\n",
        "def show_plot(imagesX, imagesY1, imagesY2):\n",
        "\timages = vstack((imagesX, imagesY1, imagesY2))\n",
        "\ttitles = ['Real', 'Generated', 'Reconstructed']\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\timages = (images + 1) / 2.0\n",
        "\t# plot images row by row\n",
        "\tfor i in range(len(images)):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(1, len(images), 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(images[i])\n",
        "\t\t# title\n",
        "\t\tpyplot.title(titles[i])\n",
        "\tpyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEeB-s5PC2XY"
      },
      "outputs": [],
      "source": [
        "show_plot(A_real, B_generated, A_reconstructed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f31kiKMXDBMT"
      },
      "source": [
        "Possiamo fare la stessa cosa da Cani a Gatti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzITSTPPDDw8"
      },
      "outputs": [],
      "source": [
        "B_real = select_sample(B_data, 1)\n",
        "A_generated  = model_BtoA.predict(B_real)\n",
        "B_reconstructed = model_AtoB.predict(A_generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm1eK7hyDGay"
      },
      "outputs": [],
      "source": [
        "show_plot(B_real, A_generated, B_reconstructed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <center>Approfondimenti sul perchè non è stata usata, limitazioni, possibili soluzioni</center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La cycleGAN non è stata utilizzata perchè, come viene citato in diversi articoli che spiegherò tra poco, una cycleGAN non performa bene quando ci sono in gioco variazioni di forma o geometriche."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ci sono numerose altre alternative all'uso della CycleGAN per l'obiettivo che mi ero prefissato, una di queste è <a href=\"https://arxiv.org/abs/2002.10102\" >GANHopper: Multi-Hop GAN for Unsupervised Image-to-Image Translation</a> che potrebbe essere riassunto come \"Quale cane apparirebbe simile dato un gatto?\".\\\n",
        "![GANHopper](https://i.ibb.co/6P98Hmj/1.png)\\\n",
        "Una GANHopper quello che fa è, anzichè fare traduzioni DIRETTE da un dominio ad un altro (per esempio quello che fa la cycleGAN) che non permette l'acquisizione delle variazioni geometriche necessarie, trasformare da un dominio ad un altro facendo degli \"hop\" ossia passando a traduzioni intermedie dell'immagine. Anche in questo caso non abbiamo coppie di immagini accoppiate da un dominio all'altro (unpaired image-to-image translation).\n",
        "Tutti gli Hop sono prodotti usando un singolo generatore lungo ogni direzione, oltre alle loss tipiche della GAN e CycleGAN, abbiamo un nuovo discriminatore chiamato \"Hybrid discriminator\" che è addestrato per classificare le immagini generate nei vari hop. Una GANHopper eccelle nella traduzione di immagini in cui ci sono variazioni geometriche come nel passaggio da cani a gatti e viceversa, sarebbe potuta essere forse il modello adatto al nostro obiettivo. Nell'articolo è citato esplicitamente come la CycleGAN non riesca a trasformare le caratteristiche geometriche ma solamente alterazioni della texture e colore a livello di pixel. Una GANHopper è costruita sopra una CycleGAN.\\\n",
        "Per esempio:\\\n",
        "Una rete four-hop per tradurre da cani a gatti produce tre immagini intermedie\n",
        "- 25% gatto, 75% cane\n",
        "- 50% gatto, 50% cane\n",
        "- 75% gatto, 25% cane\n",
        "- 0% gatto, 100% cane <- Final Hop!\n",
        "\n",
        "La capacità della rete non eccede quella di una cycleGAN e l'hybrid discriminator è addestrato solamente su immagini reali per valutare le immagini generate nei vari hop. Vengono introdotte due nuove loss oltre quelle tipiche della cycleGAN, la hybrid loss e la smoothness loss.\n",
        "\n",
        "- Hybrid Loss : per valutare il grado di appartenenza di un'immagine a uno dei domini di ingresso\n",
        "- Smoothness Loss: che regola ulteriormente le transizioni dell'immagine per garantire che un'immagine generata nella sequenza di hop non si discosti molto dall'immagine precedente.\n",
        "\n",
        "![GanHopper 2 - Hop](https://i.ibb.co/XWnz0sk/2.png)\n",
        "\n",
        "Il framework multi-hop è formato da due generatori uguali alla CycleGAN e tre discriminatori, due dei quali anch'essi uguali a quelli della CycleGAN. Il terzo discriminatore è quello citato sopra, l'hybrid discriminator.\\\n",
        "Una comparativa con altri modelli tra cui la cycleGAN\n",
        "\n",
        "![Ganhopper, comparativa con altri modelli](https://i.ibb.co/SBZGHJy/3.png)\n",
        "\\\n",
        "\\\n",
        "Un altro articolo <a href=\"https://link.springer.com/chapter/10.1007/978-3-030-70665-4_92\">A Survey on Data Augmentation Methods Based on GAN in Computer Vision</a> spiega come la DA tradizionale ha un impatto molto limitato nel avere una diversità maggiore nel dataset, le cycleGAN riescono a incrementare la diversità del dataset trasferendo lo stile dell'immagine tra immagini. Viene scritto inoltre che i metodi basati su GAN per la DA si dividono in tre tipi, i primi generano esempi da uno spazio latente (per esempio CGAN, ACGAN, TripleGAN, infoGAN ecc.), i secondi fanno una traduzione dell'immagine e i terzi producono immagini usando una strategia avversaria. Viene spiegato anche il modello Pix2Pix su cui si basa la cycleGAN, che altro non è che una CGAN in cui anzichè condizionare sulle label, condizioniamo sull'immagine in input per restituire immagini target; tant'è che la cycleGAN non ha bisogno di dati accoppiati tra i due domini di traduzione, mentre Pix2Pix si. Una cycleGAN viene in aiuto per la risoluzione dei limiti del modello CGAN Pix2Pix. Per risolvere i limiti di Pix2Pix e cycleGAN, riguardo al fatto che traducono tra DUE domini, è stato creato il modello StarGAN che permette di tradurre immagini da più domini. Infine nella conclusione è specificato come i metodi di DA basati su GAN sono efficaci in molti campi, tuttavia è necessario avere molti dati (cosa che noi non avevamo).\n",
        "\\\n",
        "\\\n",
        "Un altro articolo <a href=\"https://arxiv.org/pdf/2003.00273.pdf\">Reusing Discriminators for Encoding:\n",
        "Towards Unsupervised Image-to-Image Translation</a> mostra come diversi sforzi sono stati fatti per trasformare immagini da un dominio ad un altro per diversi task come coloramento di un'immagine, editing immagini, migliorare risoluzione ecc senza la necessità di coppie di immagini tra un dominio ed un altro, la cycleGAN viene citata come un esempio di un modello che viene usato per fare questo. In questo articolo, si chiedono se si può ripensare il ruolo di ogni componente in una GAN (prevalenetemente, anche se non indicato, solamente il ruolo di generatore e discriminatore). In questo articolo viene proposto di riusare il discriminatore per fare l'encoding, si usano i primi layer nel discriminatore come encoder del dominio target, in questa maniera riusciamo ad avere un'architettura più compatta dato che l'encoder diventa parte del discriminatore e non abbiamo bisogno di un componente indipendente per l'encoding. In più, nel momento in cui si deve fare inferenza, la parte usata per l'encoding del discriminatore è tenuta per fare inferenza. In questo modo l'encoder è addestrato in maniera più efficace, infatti in genere l'addestramento dell'encoder è fatto facendo la backpropagation dei gradienti dal generatore, collegando l'encoder al discriminatore, l'encoder è addestrato direttamente attraverso la loss del discriminatore. \n",
        "Dato che l'encoder e il discriminatore si sovrappongono, questo comporta un'instabilità se applichiamo un training setting tradizionale (adversarial), l'encoder come parte della traduzione viene addestrato per minimizzare e allo stesso tempo appartiene al discriminatore e viene quindi anche addestrato per massimizzare. Per risolvere quest'ultimo problema, l'addestramento è disaccoppiato. L'addestramento dell'encoder è associato solo al discriminatore indipendentemente dal generatore; gli esperimenti hanno mostrato che questo disaccoppiamento favorisce notevolmente l'addestramento. Viene mostrato come nella traduzione da cane a gatto, si hanno performance più del doppio migliori rispetto ad una CycleGAN.\n",
        "![.](https://i.ibb.co/23tW4rd/3.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un fatto che secondo me ha senso da far notare è come le cycleGAN possono essere molto utili in campo medico. Infatti, come citato nel blog creato dai creatori della cycleGAN <a href=\"https://junyanz.github.io/CycleGAN/\">cycleGAN Blog</a>, una CycleGAN può trasformare un'immagine in un altra in cui magari è presente un tumore per mostrare come sarebbe, o viceversa nel caso in cui si volesse mostrare la non presenza di un tumore. Oppure può avere altri usi come il colorare le immagini, come citato sempre nel blog ufficiale. Ci sono anche altri usi per cui un modello come le CycleGAN risulta essere molto utile, nonostante le sue limitazioni."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ci sono altri articoli, ho citato i principali che penso abbiano maggior importanza."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il codice in questione è stato scelto da diversi tutorial online per leggibilità sulla base di quello visto a lezione di teoria. La maggior parte erano implementazioni in PyTorch o tensorflow ma non facilmente spiegabili come questo."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
